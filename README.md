# AwsomeFS â€“ A Cluster-Aware Distributed Filesystem for Kubernetes

**AwsomeFS** is an experimental, shared-block distributed filesystem designed for Kubernetes clusters. It combines shared device coordination, metadata-driven consistency, and CSI integration to offer a developer-friendly yet enterprise-aware platform for building and running stateful workloads with fine-grained control over storage behavior.

---

## âš¡ï¸ Why AwsomeFS?

Traditional distributed filesystems like Ceph, GlusterFS, and Longhorn are proven and scalableâ€”but can be complex to operate, tightly coupled, and opaque to extend or debug.

**AwsomeFS explores a middle-ground**:

- You already have **shared block devices** (SAN, iSCSI, NVMe-oF, loopback).
- You want **explicit coordination and consistency**â€”without a heavyweight replication layer.
- You need **Kubernetes-native integration**, with a pluggable, understandable stack.
- You prefer **transparency and modularity**, enabling customization and rapid iteration.
- You want **language-modern implementations** (Rust & Go), built for security and observability.

---

## ğŸ”© Key Features

- ğŸ§  **Metadata Service**: Distributed lock and coordination layer to manage volume ownership, fencing, and lifecycle.
- ğŸ’¡ **fs-core (agent)**: Local daemon for volume IO coordination, device initialization, and node-level hooks.
- ğŸ“¦ **CSI Plugin**: Seamless integration with Kubernetes, supporting provisioning, attachment, and lifecycle flows.
- ğŸ§ª **Kind-based Dev Cluster**: Develop and test with raw devices simulated via loopbackâ€”ideal for rapid prototyping.
- ğŸ”§ **Optional Kernel Module**: Enables enhanced FS primitives or custom device handling for advanced use cases.
- ğŸ§± **Hybrid Architecture**: Centralized metadata with decentralized IO paths = low coordination overhead.

---

## ğŸ§° When to Use This (vs Existing Solutions)

| Use Case | AwsomeFS | Ceph/Gluster | Local Volumes |
|----------|-------|---------------|----------------|
| Shared block device already exists | âœ… | âœ… | âŒ |
| Replication/Erasure Coding | ğŸš« | âœ… | ğŸš« |
| Transparent and hackable design | âœ… | âŒ | âœ… |
| Lightweight deployment | âœ… | âŒ | âœ… |
| Custom IO semantics / research | âœ… | ğŸš« | ğŸš« |
| Kubernetes CSI integration | âœ… | âœ… | âœ… |

---

## Comparative Overview: AwsomeFS vs Lustre vs GPFS vs Ceph

| Feature / System                        | AwsomeFS                          | Lustre                              | GPFS (Spectrum Scale)              | Ceph                                |
|----------------------------------------|-----------------------------------|-------------------------------------|-------------------------------------|-------------------------------------|
| **Shared Raw Device Support**          | âœ… Yes (via loop or SAN device)   | âœ… Yes                               | âœ… Yes                               | âŒ Not typical; object/block-based   |
| **Kubernetes CSI Integration**         | âœ… Native (built-in CSI driver)   | âŒ No official CSI                  | âš ï¸ Limited / complex integration    | âœ… Mature CSI driver                 |
| **Runs in Userspace**                  | âœ… Fully                           | âŒ Kernel modules required           | âŒ Kernel modules required           | âœ… Mostly (except for kernel RBD)    |
| **POSIX-Compliant Filesystem**         | âœ… Yes                             | âœ… Yes                               | âœ… Yes                               | âš ï¸ Not directly (CephFS via MDS)     |
| **Data / Metadata Separation**         | âœ… Cleanly separated               | âœ… Yes                               | âœ… Yes                               | âœ… Yes (CephFS uses MDS)             |
| **Multi-node Concurrent Access**       | âœ… Designed for it                 | âœ… Yes                               | âœ… Yes                               | âš ï¸ RBD is block-only, no shared FS   |
| **Development-Friendly**              | âœ… Kind/dev-env support           | âŒ Not dev-oriented                 | âŒ Very enterprise/HPC oriented     | âœ… Yes                               |
| **Lightweight / Containerizable**      | âœ… Fully containerized            | âŒ No                                | âš ï¸ Partial (heavyweight base)       | âœ… Yes                               |
| **Kernel Dependencies**                | âŒ None                            | âœ… Required                          | âœ… Required                          | âš ï¸ RBD requires kernel driver        |
| **Enterprise-Readiness**               | âš ï¸ In-progress (viable roadmap)   | âœ… HPC-focused                      | âœ… Enterprise-grade                 | âœ… Widely adopted                    |
| **License**                            | MIT or similar (custom)           | GPL / Open Source                   | Commercial (IBM)                    | LGPL / Open Source                  |

---

## Summary

- **AwsomeFS** gives developers and operators **Kubernetes-native shared-disk access** with modern container tooling and simplicity.
- **Lustre** and **GPFS** offer **high performance and concurrency**, but are not dev-friendly, and require **kernel-level changes** and **complex deployment**.
- **Ceph** excels in **cloud-native storage**, but itâ€™s fundamentally object/block-based and not ideal for **raw shared disks** with filesystem-level control.

---

## Use AwsomeFS If You Want:
- To develop or run on **Kubernetes with shared block devices**
- **Dynamic PVC creation** from shared disks
- Container-based deployment
- A path toward an enterprise-grade solution with dev-first mindset



## ğŸ¢ Enterprise-Oriented Use Cases

While experimental in nature, **AwsomeFS opens a pathway** to enterprise scenarios such as:

- **Cloud-Adjacent Deployments**: Simplify access to SAN-like devices across Kubernetes workloads with clear coordination guarantees.
- **Edge Clusters**: Where low-resource nodes share physical devices, but full-scale distributed storage is overkill.
- **Performance-Sensitive Applications**: Bypass userspace proxies and replicate logic to leverage hardware-accelerated shared storage.
- **Security & Compliance**: Use Rust and Go to build auditable, memory-safe components.
- **Custom SLA Environments**: When you control the hardware and want policy-enforced access control at the FS layer.

> ğŸ’¡ You get Kubernetes-native volumes, advanced isolation, and performanceâ€”**without** depending on full-scale storage vendors or black-box systems.

---

## ğŸ§ª Project Status

> âš ï¸ **Experimental**. AwsomeFS is not production-ready yet. Ideal for prototyping, teaching, custom infra builds, and exploring FS design space.

---

## ğŸš€ Getting Started

```bash
make dev-env  # Sets up kind cluster with loopback-based raw devices
make build    # Builds fs-core, metadata-service, csi-driver
make deploy   # Deploys everything into your dev cluster
```

Loopback setup is handled inside `dev-env/kind-config.yaml`.

---

## ğŸ“¦ Project Structure

```
/
â”œâ”€â”€ cmd/                      # Entrypoints for binaries
â”‚   â”œâ”€â”€ fs-core/              # Filesystem agent (Rust)
â”‚   â”œâ”€â”€ metadata-service/     # Metadata coordination service (Rust)
â”‚   â””â”€â”€ csi-driver/           # Kubernetes CSI driver (Go)
â”œâ”€â”€ internal/                 # Shared libraries and modules
â”œâ”€â”€ kernel/                   # Optional kernel module (Rust + C)
â”œâ”€â”€ k8s/                      # Helm charts and manifests
â”œâ”€â”€ dev-env/                  # Kind config, loopback setup
â”œâ”€â”€ Makefile                  # Build and development automation
```

---

## ğŸ§  Philosophy

AwsomeFS is not here to replace production-grade storage platforms.

Instead, it's a **transparent, modular toolkit** for developers, infra engineers, and researchers to explore how far simple coordination + shared devices can goâ€”with a clean Kubernetes-native interface and modern dev stack.
